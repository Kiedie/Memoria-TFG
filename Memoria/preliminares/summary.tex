% !TeX root = ../libro.tex
% !TeX encoding = utf8
%
%*******************************************************
% Summary
%*******************************************************

\selectlanguage{english}
\chapter{Summary}

    \begin{center}
        \textbf{Key words:}
        \begin{center}
            Atrial Fibrillation, Arrhythmia, Deep Learning, Classification, Artificial Intelligence. 
        \end{center}
    \end{center}

    Artificial intelligence has been one of the great milestones of the last century and after countless obstacles, today it is an indissoluble part of our daily lives. So much so that if it hypothetically ceased to exist, the world would collapse. Today it can be seen everywhere, autonomous driving systems, online text translators, smart watches and even in some household appliances. Artificial intelligence is not only used to provide human comfort, but also to save lives, and this is the main focus of the work. The project is based on the application of artificial intelligence, more specifically deep learning, to detect and classify a type of heart disease known as arrhythmia. \\

    Although the document has been developed in 5 parts, it can be seen as three main parts: mathematics, computer science and project development. The first two are the most theoretical, leaving the last one for practical application. The first part introduces the mathematical foundations behind machine learning. In particular, the work focuses on describing the foundations of statistical learning, previously detailing its pillars in order to progressively build this theory. Statistical learning is based on probability, which in turn requires concepts of measurement theory and integration. Thus, we begin by giving some fundamentals of analysis that are necessary in what follows. The key concepts of measure theory to build this framework will be discussed in detail below, since a measurable space with an associated measure forms what is known as a measure space, which is a particular case of a probability space. Some concepts about integration will also be of vital importance as they will be used later to explain the calculation of probabilities. \\

    After detailing the most basic pillars, the theoretical framework of modern probability will be gradually introduced. The most important idea of this theory is to model one or more random experiments in a probability space. In most cases, the observations are nondeterministic and will be encoded as measurable applications. Thus, each random observation is a measurable application and the probabilities of the entire range of possible observations will be described in terms of the distribution of the corresponding random variable. Here, the aforementioned concepts of random variable, distribution function, density function, probability mass function plus some properties will be defined. At this point, the theory of measurement is left behind because it cannot describe the structure of dependence existing between random variables and will enter fully into the theory of probability. We will begin by defining the dependence between events and end with the dependence of random variables linked to the conditional probability. Then, we will go into detail with one of the most important and fundamental concepts, the moments, in particular we will talk about hope, variance and covariance. We conclude the part of probability with one of the most relevant results in statistical learning theory, the famous Hoeffding inequality. This probabilistic inequality bounds the probability that the mean of a set of independent and identically distributed random variables deviates by more than one threshold with respect to its expectation. \\



    Having explained and understood all of the above, it is time to dive into the theory of statistical learning, we will see some general results, but most of the work is focused on a particular and especially simple case, the binary classification. We will begin by defining the concepts that make up the learning task, all with the objective of formally defining the concept of learning. Next, the performance paradigm, known as Empirical Error Minimization (ERM), will be discussed. Then the concept of Probabilistically Approximate Correct Learning (PAC) will be introduced, which allows to evaluate the difficulty of a problem in the context of supervised learning. This difficulty is evaluated at two levels, whether the learning algorithm or the hypothesis class is PAC learnable or not. Therefore, after the definition of the concept, the conditions under which a model fulfills this property will be studied. Later it will be seen that the definition of learnable CAP is not entirely general and an effort will be made to generalize it, which gives rise to the concept of agnostic CAP learning. After which we will proceed again to find the conditions under which a model is CAP agnostic and for this purpose we will resort to the concept of learning by uniform convergence. \\
    
    Continuing still in the framework of statistical learning, a leap is made to the study of generalization levels. Generalization scores are a very important tool that measure how well your model has managed to transcend learning to achieve correct results outside the training data. We will classify the generalization scores according to the hypothesis set, so we will have generalization scores for finite hypothesis sets, distinguishing two cases, the consistent and the inconsistent; and for infinite hypothesis sets, and it is in this part where the Vapnik-Chervonenkis dimension theory, better known as VC dimension, begins. \\
    
    To go into this theory we will first define some key concepts such as dichotomy, growth function and breakpoints. The concept of VC dimension will then be defined. The study of learnable PAC models was carried out for those with finite hypothesis classes, however, when the hypothesis class is infinite, it is at this point when the VC dimension is used, since infinite hypothesis sets with finite VC dimension can exist, which allows the study of a wide range of models. Furthermore, it will be seen that when the VC dimension is infinite, then the model will not be PAC learnable, so the object of study will now focus on those models with finite VC dimension regardless of the cardinal of the hypothesis class. After the staging of this theoretical framework, we continue by detailing the generalization dimension for infinite hypothesis class known as the VC dimension. This quotient is one of the most important mathematical results in statistical learning theory for establishing the feasibility of learning with infinite set of hypotheses and for being a universal result, being understood that it is applicable to any learning algorithm, any set of hypotheses, etc. This section will conclude with the fundamental theorem of statistical learning, which relates all the concepts discussed so far. \\
    
    VC analysis shows that the choice of a hypothesis has to strike a balance between approximation to the target function within the training data and generalization to the new data. Another viewpoint that addresses this is the approximation-generalization trade-off, which is based on the decomposition of the expectation of the squared error into two terms known as bias and variance. \\
    
    To conclude the mathematical part of the project, the universal approximation theorem will be introduced, which states that the output produced by a neural network is dense in the space of continuous functions in any compact interval. In other words, we can approximate any continuous function in a compact space by means of neural networks. This result is supported by two important theorems of functional analysis: the Hanh-Banach theorem and the Riesz representation theorem. \\
    
    
    
    The second part of the project has a theoretical computer science approach with the objective of understanding the mechanisms applied in the third part of the project, which is based on the elaboration of Deep Learning models based on convolutional neural networks and LSTM to solve a problem. This part will begin with a brief state of the art of neural networks. Then we will introduce the pre-fed neural networks and the notation to be used will be fixed. Neural networks are so broad that they have not been covered in their entirety, but one of the most important parts, training, has not been omitted either. Having said that, we will detail the training process, which consists of two stages. The first one is called forward propagation, in which the loss function is calculated. This stage is so named because the weights of one layer influence the weights of the next layer, causing the propagation to advance in the direction of the first layers towards the last ones. The second stage is called back propagation and is where the well-known Back-Propagation algorithm is applied. In this stage the gradient of the loss function is calculated as a function of the weights. The name of this stage is due to the fact that in the calculation of the gradient is advanced from the last layers to the first ones. After the gradient calculation, the optimization method known as downward gradient, which has many variants, is applied. Another key aspect of neural networks is the type of activation function thanks to which a non-linearity contribution to the output is achieved, the most common activation functions used today will be discussed and compared with each other, in addition to mentioning disadvantages and disadvantages.
    
    
    After knowing the modus operandis of pre-fed neural networks, convolutional neural networks are introduced, which are simply the previous ones but including a set of new layers, not only neurons. This type of networks emerged with the purpose of identifying patterns or targets in images, but they also have a wide application in one-dimensional signals. Pre-powered neural networks have the disadvantage that if they operate with images, the amount of weights that the network needs to train increases considerably, making them not very functional. Convolutional networks solve this and other problems. To understand the operation of these networks, we must first understand the convolution operation and how it works. Basically it consists of sliding a filter over the input performing basic operations. Once this is understood, the layers that exist in this type of networks will be presented. We will talk about convolutional layers and some of their properties such as local connectivity, spatial arrangement and the implications of sharing parameters. The role and usefulness of pooling layers and normalization layers will also be discussed, and this section will end with a brief explanation of the design patterns of these network architectures. \\
    
    To conclude the second part of this project, we will look at a type of neural networks that is quite important for the third part, recurrent neural networks (RNN). These types of networks exhibit behavior that is somewhat intelligent in that they have the ability to remember and forget information over time, which is really useful. There are many types of recurrent neural networks, in particular this section will focus on those known as LSTM which is a variation of this type of networks that solve a very common problem they have, and the short and long term dependence caused by gradient fading and/or bursting. The structure of an LSTM cell is quite curious, it is formed by a set of gates, which are nothing more than an activation function with an associated operation (addition or multiplication) that perform different tasks. For example, one gate is in charge of what to forget, another one decides what to remember, another one is in charge of putting together the current knowledge with what to forget and what to remember. GRUs are another type of recurrent neural networks, similar to LSTMs but with a different structure. This concludes the second part of the project. \\
    
    
    
    The third part of the project culminates with the development of the practical part where a problem of great relevance today is presented. Arrhythmia is a disease caused by an irregular heartbeat, which is a serious health problem and in the worst case, death. This pathology does not understand gender, it affects men and women equally and although it is present in older individuals, it can also be found in the young population. Health specialists diagnose arrhythmia by analyzing the signals provided by an electrocardiogram (ECG) which measures the electrical activity of the heart. Automating the detection and classification of arrhythmias is of vital importance not only for patients, but also for physicians. \\
    
    The traditional approach to this problem has been for researchers to extract signal features manually. This has a drawback, which is that a priori medical knowledge is required, increasing the level of difficulty. With the development of deep learning in recent years, this was no longer a problem, as it was the model itself that was responsible for extracting the features it saw fit. It is this latter approach to which the work is devoted. For the experimentation we used a database of a competition carried out by the PhysioNet platform in which the objective was to classify a type of arrhythmia called \textit{atrial fibrillation}. The models that have been used to classify arrhythmias are of two types, the first are models already proposed by the literature, which have been replicated and tested. The second type is a set of three proprietary proposals obtained after an experimental process. The first proposal consisted of a standard convolutional network architecture with nothing to highlight. The second proposal was a combination of CNN and LSTM. The last proposal is a variant of the second one but changing the LSTM units for GRU. \\
    
    As expected, the best results were those of the winning models of the competition. The worst results were presented by those models extracted from the literature for a simple reason, overfitting. This is because these models had many more degrees of complexity than the data required, thus overfitting to the peculiarities of the inherent noise. In the end, the in-house models, while not outperforming the winners, performed well enough to compete. \\



% Al finalizar el resumen en inglés, volvemos a seleccionar el idioma español para el documento
\selectlanguage{spanish} 
\endinput
